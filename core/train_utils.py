import sys
import random
import tensorflow as tf
from metric import get_sparsity_loss, get_continuity_loss, compute_accuracy
from visualize import show_binary_rationale


def gen_nl_loss(logits, targets, truth, path):
    """
    This is a negative likelihood loss for the generator. 
    In other words, generator max the prob of the targets. 
    Note that, likelihood does not take log.  
    Inputs: 
        logits -- the logits from the disc. (batch_size, num_classes)
        targets -- either all zeros or all ones, 
                   depends on which path it go through.
        truth -- the real label of each examples,
                  this is not used to calculate the loss. 
        path -- whether the rationale is generated by G0 or G1.        
    """
    softmax_probs = tf.nn.softmax(logits=logits, axis=-1)
    probs = tf.reduce_sum(softmax_probs * tf.cast(targets, tf.float32), axis=-1)
    total_loss = -tf.reduce_mean(probs)

    return total_loss


def train(model, optimizers, dataset, step_counters, args):
    """
    Training target dependent rationale generation 
    (Tommi's three player version).
    """
    gen_pos_optimizer = optimizers[0]
    gen_neg_optimizer = optimizers[1]
    dis_optimizer = optimizers[2]

    gen_pos_step_counter = step_counters[0]
    gen_neg_step_counter = step_counters[1]
    dis_step_counter = step_counters[2]

    for (batch, (inputs, masks, labels)) in enumerate(dataset):
        # get variables
        gen_pos_vars = model.generator_pos_trainable_variables()
        gen_neg_vars = model.generator_neg_trainable_variables()
        dis_vars = model.discriminator_trainable_variables()

        # construct the target labels for the generator
        batch_size = inputs.shape[0]
        all_ones = tf.ones([batch_size, 1], tf.int32)
        all_zeros = tf.zeros(all_ones.shape, tf.int32)

        path = dis_step_counter.numpy() % 2

        if path == 0:
            # go through the G0
            gen_targets = tf.concat([all_ones, all_zeros], axis=-1)
        else:
            # go through the G1
            gen_targets = tf.concat([all_zeros, all_ones], axis=-1)

        with tf.GradientTape() as dis_tape:
            # logits -- (batch_size, num_classes)
            # rationales -- (batch_size, seq_length, 2)
            dis_logits, rationales = model(inputs, masks, labels, path)
            dis_losses = tf.nn.softmax_cross_entropy_with_logits_v2(
                logits=dis_logits, labels=labels)
            dis_loss = tf.reduce_mean(dis_losses)

        # compute graident for the disc
        dis_grads = dis_tape.gradient(dis_loss, dis_vars)

        with tf.GradientTape() as gen_tape:
            dis_logits, rationales = model(inputs, masks, labels, path)

            # generator loss
            class_loss = gen_nl_loss(dis_logits, gen_targets, labels, path)
            sparsity_loss = args.sparsity_lambda * get_sparsity_loss(
                rationales[:, :, 1], tf.cast(masks, tf.float32),
                args.sparsity_percentage)
            continuity_loss = args.continuity_lambda * get_continuity_loss(
                rationales[:, :, 1])
            gen_loss = class_loss + sparsity_loss + continuity_loss

        if path == 0:
            gen_neg_grads = gen_tape.gradient(gen_loss, gen_neg_vars)
            gen_neg_optimizer.apply_gradients(zip(gen_neg_grads, gen_neg_vars),
                                              global_step=gen_neg_step_counter)
        else:
            gen_pos_grads = gen_tape.gradient(gen_loss, gen_pos_vars)
            gen_pos_optimizer.apply_gradients(zip(gen_pos_grads, gen_pos_vars),
                                              global_step=gen_pos_step_counter)

        dis_optimizer.apply_gradients(zip(dis_grads, dis_vars),
                                      global_step=dis_step_counter)

        # compute accuarcy
        dis_accuracy = compute_accuracy(dis_logits, labels)

        if (args.visual_interval and
            ((batch + 1) % args.visual_interval in [0, 1]) and batch != 0):

            batch_size = inputs.get_shape()[0]
            # random a sample from the batch_size
            sample_id = random.randint(0, batch_size - 1)

            print('Batch # %d: sample id: %d, true label: %d, path: %d' %
                  (batch + 1, sample_id, tf.argmax(labels[sample_id, :]), path))
            show_binary_rationale(inputs[sample_id, :].numpy(),
                                  rationales[sample_id, :, 1].numpy(),
                                  args.idx2word)
            sys.stdout.flush()
